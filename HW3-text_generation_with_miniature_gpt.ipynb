{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZxD7N0kzqWZ"
      },
      "source": [
        "# Text generation with a miniature GPT\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/29<br>\n",
        "**Last modified:** 2020/05/29<br>\n",
        "**Description:** Implement a miniature version of GPT and train it to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNVBaTr2zqWg"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement an autoregressive language model\n",
        "using a miniature version of the GPT model.\n",
        "The model consists of a single Transformer block with causal masking\n",
        "in its attention layer.\n",
        "We use the text from the IMDB sentiment classification dataset for training\n",
        "and generate new movie reviews for a given prompt.\n",
        "When using this script with your own dataset, make sure it has at least\n",
        "1 million words.\n",
        "\n",
        "This example should be run with `tf-nightly>=2.3.0-dev20200531` or\n",
        "with TensorFlow 2.3 or higher.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n",
        "- [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "- [GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik5oNo61zqWj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4UMCqw9LzqWl"
      },
      "outputs": [],
      "source": [
        "# We set the backend to TensorFlow. The code works with\n",
        "# both `tensorflow` and `torch`. It does not work with JAX\n",
        "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
        "# (used in `causal_attention_mask()`: `tile` in JAX does\n",
        "# not support a dynamic `reps` argument.\n",
        "# You can make the code work in JAX by wrapping the\n",
        "# inside of the `causal_attention_mask` function in\n",
        "# a decorator to prevent jit compilation:\n",
        "# `with jax.ensure_compile_time_eval():`.\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import tensorflow\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNTLYuawzqWo"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zqNah_O9zqWp"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = ops.arange(n_dest)[:, None]\n",
        "    j = ops.arange(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = ops.cast(m, dtype)\n",
        "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = ops.concatenate(\n",
        "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
        "    )\n",
        "    return ops.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6SXuEOuzqWq"
      },
      "source": [
        "## Implement an embedding layer\n",
        "\n",
        "Create two separate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "34q9patnzqWr"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(0, maxlen, 1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knP90MMezqWt"
      },
      "source": [
        "## Implement the miniature GPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q7sRdGSUzqWu"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\",\n",
        "        loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff-sz9GrzqWv"
      },
      "source": [
        "## Prepare the data for word-level language modelling\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v4opVmskzqWv",
        "outputId": "9b1e0151-0af0-4706-f16f-581af1a5927a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  61.8M      0  0:00:01  0:00:01 --:--:-- 61.8M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n0D8IaHCzqWv",
        "outputId": "56c75abc-3748-4ea4-c5bb-16d663604e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "# Create a dataset from text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf_data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf_strings.lower(input_string)\n",
        "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tensorflow.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80pvURAtzqWw"
      },
      "source": [
        "## Implement a Keras callback for generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VZhzryQnzqWw"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x, verbose=0)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHzYzCGkzqWy"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "yK5inuJrzqWz",
        "outputId": "8205e17c-62b1-4177-d8bd-cbcbbcca59d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text:\n",
            "this movie is a movie and a lot of people . i would say that i am a huge fan of this movie . i was expecting it was very bad . and i have seen . i would be a few minutes of\n",
            "\n",
            "391/391 - 72s - 183ms/step - loss: 5.4659\n",
            "Epoch 2/25\n",
            "generated text:\n",
            "this movie is not funny . the plot has no plot , the film was not very interesting . i had a lot of good acting , and it is just plain boring . i have to say about it ! i thought it\n",
            "\n",
            "391/391 - 55s - 140ms/step - loss: 4.7010\n",
            "Epoch 3/25\n",
            "generated text:\n",
            "this movie is so bad , and just so bad . it is the worst i have ever seen in . it looks like a movie , and a movie about how it 's the audience is a real -life movie , so ,\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 4.4521\n",
            "Epoch 4/25\n",
            "generated text:\n",
            "this movie is not a typical of the [UNK] ' movie . if you 're not expecting much to see , it might be a good movie . the acting is terrible and it 's a great movie . the characters are very realistic\n",
            "\n",
            "391/391 - 59s - 151ms/step - loss: 4.2959\n",
            "Epoch 5/25\n",
            "generated text:\n",
            "this movie is so bad in a small town where the women are going to be [UNK] , as a teen , [UNK] [UNK] girl \" in the middle of her family and the girl 's father . she is not in love ,\n",
            "\n",
            "391/391 - 59s - 151ms/step - loss: 4.1757\n",
            "Epoch 6/25\n",
            "generated text:\n",
            "this movie is so boring , it 's an amazing look to it . it 's a shame that 's not even a good movie . it 's a great film with lots of great acting . i 'm not sure how much it\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 4.0775\n",
            "Epoch 7/25\n",
            "generated text:\n",
            "this movie is so bad in its bad , the plot is stupid and predictable . the acting is bad . the movie looks like a porno flick , the plot is just plain bad , but it is awful . the worst film\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 3.9927\n",
            "Epoch 8/25\n",
            "generated text:\n",
            "this movie is a disgrace . the acting is poor , and the story of a [UNK] of [UNK] 's book [UNK] , the book , the movie was very poor and it really had to be one of the worst special effects and\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 3.9195\n",
            "Epoch 9/25\n",
            "generated text:\n",
            "this movie is a great example of how bad the script was ? the first of all i thought that the actors did a good job . the story line was excellent , the writing was excellent . the acting was good and the\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 3.8544\n",
            "Epoch 10/25\n",
            "generated text:\n",
            "this movie is one of the worst movies i have ever seen , and i don 't believe it is not . i can be impressed with the entire film . the acting is not even though i was not expecting much from the\n",
            "\n",
            "391/391 - 59s - 151ms/step - loss: 3.7956\n",
            "Epoch 11/25\n",
            "generated text:\n",
            "this movie is one of all movies that [UNK] a lot . it should have been made out for it . it has been made in years before it was not the same . this was a very popular movie because it showed a\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.7421\n",
            "Epoch 12/25\n",
            "generated text:\n",
            "this movie is one of the biggest tragedies of the [UNK] that have ever done so in a movie that you will not get into the movie . this movie has a very heart , a movie that is a great movie , it\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.6929\n",
            "Epoch 13/25\n",
            "generated text:\n",
            "this movie is very good but very good acting . the movie looks great and the actors do nothing in a movie . the storyline goes nowhere , but the plot is thin . . i guess what happens . . . the story\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.6481\n",
            "Epoch 14/25\n",
            "generated text:\n",
            "this movie is a very strange , but it is a good example . the [UNK] of the movie is very bad , and very funny , very funny . i thought that this movie was good , the first movie i saw it\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.6070\n",
            "Epoch 15/25\n",
            "generated text:\n",
            "this movie is just another one of the best movies i 've seen ! but it was not a perfect example of what the movie was that the actors made it a little . the movie is a very realistic depiction of events shown\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.5683\n",
            "Epoch 16/25\n",
            "generated text:\n",
            "this movie is the best example of the word [UNK] ' and [UNK] [UNK] . the acting is amazing . it 's the worst film i have ever seen . it 's a lot worse . . .the acting was worse than the special\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.5335\n",
            "Epoch 17/25\n",
            "generated text:\n",
            "this movie is a great story of the movie . a great cast , and a story of a young orphan named john cromwell who develops a small roles as a former star . the supporting cast was good for the film [UNK] \"\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.5004\n",
            "Epoch 18/25\n",
            "generated text:\n",
            "this movie is so horrible , and terrible . it 's not just one to see how bad movies this one is ? this movie is a horrible piece of crap i would want to get out of it . the script is not\n",
            "\n",
            "391/391 - 59s - 150ms/step - loss: 3.4692\n",
            "Epoch 19/25\n",
            "generated text:\n",
            "this movie is so bad it 's just a few things about the story is a little slow , but nothing special . there are some things going on . it is not a bad plot , but it 's just plain fun .\n",
            "\n",
            "391/391 - 59s - 151ms/step - loss: 3.4420\n",
            "Epoch 20/25\n",
            "generated text:\n",
            "this movie is a great example of the acting of a story and the story . the story begins slowly and the characters are a knockout . they are trained on her own . the story is about a woman who gets pregnant with\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 3.4151\n",
            "Epoch 21/25\n",
            "generated text:\n",
            "this movie is the worst acting i 've ever seen . i 've seen a worse movie . i was looking forward to this movie . i was looking at my car watching this movie . . i can 't say the worst movies\n",
            "\n",
            "391/391 - 60s - 152ms/step - loss: 3.3908\n",
            "Epoch 22/25\n",
            "generated text:\n",
            "this movie is about one of the worst movie ever . the only way i was watching it i couldn 't really laugh and cry at all the same . the acting was horrible , the only redeeming factor was [UNK] [UNK] 's [UNK]\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 3.3673\n",
            "Epoch 23/25\n",
            "generated text:\n",
            "this movie is a very strange movie that has to be a bad movie but i don `t really get to see the [UNK] of the actors complaining about what happens in a movie . but then the movie is about a man in\n",
            "\n",
            "391/391 - 60s - 152ms/step - loss: 3.3451\n",
            "Epoch 24/25\n",
            "generated text:\n",
            "this movie is a complete waste of time . it was not bad . the acting was horrible . the script was awful . acting was pathetic ! the story was terrible and the acting was terrible , the acting . i don 't\n",
            "\n",
            "391/391 - 59s - 152ms/step - loss: 3.3251\n",
            "Epoch 25/25\n",
            "generated text:\n",
            "this movie is really about a young woman who is going to live , but she has to go on and is amazed by her lover , but that is what is really going on with her , the story is not only the\n",
            "\n",
            "391/391 - 60s - 152ms/step - loss: 3.3057\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7974f48342c0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model = create_model()\n",
        "\n",
        "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc9OMx0pzqW3"
      },
      "source": [
        "## Relevant Chapters from Deep Learning with Python\n",
        "- [Chapter 15: Language models and the Transformer](https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer)\n",
        "- [Chapter 16: Text generation](https://deeplearningwithpython.io/chapters/chapter16_text-generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q1 Option A — Function-Based Text Generation:\n",
        "Implement a reusable function (e.g., generate_text(...)) that:\n",
        "* Accepts a text prompt\n",
        "* Generates a configurable number of new tokens\n",
        "* Uses probabilistic sampling (not argmax)\n",
        "* Returns generated text as a string\n",
        "\n",
        "You should be able to call it like:\n",
        "* generate_text(model, \"this movie is\", num_tokens=60, temperature=0.8, top_k=20)"
      ],
      "metadata": {
        "id": "P7R7A01e-J9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating text on demand\n",
        "def generate_text(model, prompt, max_tokens=60, temperature=0.8, top_k=20):\n",
        "  \"\"\"\n",
        "  Option A — Function-Based Text Generation (Minimum Requirement)\n",
        "  Implement a reusable function (e.g., generate_text(...)) that:\n",
        "  * Accepts a text prompt\n",
        "  * Generates a configurable number of new tokens\n",
        "  * Uses probabilistic sampling (not argmax)\n",
        "  * Returns generated text as a string\n",
        "  \"\"\"\n",
        "  #starts the tokenization process (prompt into consumable bits for model)\n",
        "  start_tokens = [word_to_index.get(word, 1) for word in prompt.split()]\n",
        "  num_tokens_generated = 0 #iniliazes tokens\n",
        "  tokens_generated = [] #stores the tokens\n",
        "\n",
        "  while num_tokens_generated <= max_tokens:\n",
        "    pad_len = maxlen - len(start_tokens)\n",
        "    sample_index = len(start_tokens) - 1\n",
        "    if pad_len < 0:\n",
        "      x = start_tokens[:maxlen]\n",
        "      sample_index = maxlen - 1\n",
        "    elif pad_len > 0:\n",
        "      x = start_tokens + [0] * pad_len\n",
        "    else:\n",
        "      x = start_tokens\n",
        "    x = np.array([x])\n",
        "    y, _ = model.predict(x, verbose=0)\n",
        "\n",
        "    #add in temperature - how consistent this will be (lower=more; high=less)\n",
        "    logits = y[0][sample_index] / temperature\n",
        "    #add in top-k part of predicitive maths the \"most\" likely next token k out of the pile\n",
        "    logits, indices = ops.top_k(logits, k=top_k, sorted=True)\n",
        "    indices = np.asarray(indices).astype(\"int32\")\n",
        "    preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype(\"float32\")\n",
        "\n",
        "    sample_token = np.random.choice(indices, p=preds)\n",
        "    tokens_generated.append(sample_token)\n",
        "    start_tokens.append(sample_token)\n",
        "    num_tokens_generated += 1\n",
        "\n",
        "  generated_text = [vocab[token] for token in tokens_generated]\n",
        "  return prompt + \" \" + \" \".join(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "5oA-YzUB-H3A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q2 Sampling Controls:\n",
        "Your interaction must expose at least two of the following parameters:\n",
        "\n",
        "Parameter\tDescription\n",
        "* Temperature\tControls randomness. Lower = safer, higher = more creative\n",
        "* Top-k\tRestricts sampling to the top-k most likely tokens\n",
        "* Max tokens\tNumber of tokens generated beyond the prompt\n",
        "* You must demonstrate that changing these parameters affects output behavior."
      ],
      "metadata": {
        "id": "DftcB23Qb9oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate different answers with different parameters\n",
        "prompt = \"I like this movie because\"\n",
        "\n",
        "print(\"\\nChange in Temperature changes how consistent responses will be. Default was set to 80%.\")\n",
        "print(\"\")\n",
        "for i in range(3):\n",
        "  print(f\"Temperature at .8 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=20)}\")\n",
        "for i in range(3):\n",
        "  print(f\"Temperature at .2 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.2, top_k=20)}\")\n",
        "\n",
        "print(\"\\nChange in Top-K changes how much variety in word choice there is. Default was set to 20.\")\n",
        "print(\"\")\n",
        "for i in range(3):\n",
        "  print(f\"Top-k at 20 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=20)}\")\n",
        "for i in range(3):\n",
        "  print(f\"Top-k at 2 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=2)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xzgZHxHEs8YO",
        "outputId": "7a3556bc-f056-4b79-f60c-7dfe4c3e043f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Change in Temperature changes how consistent responses will be. Default was set to 80%.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1610988611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Temperature at .8 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=20)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Temperature at .2 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.2, top_k=20)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q3 Prompt Exploration:\n",
        "You must test your model with at least 10 distinct prompts, including:\n",
        "\n",
        "Short prompts (2–4 words)\n",
        "\n",
        "Medium prompts (5–8 words)\n",
        "\n",
        "At least one ambiguous or incomplete prompt\n",
        "\n",
        "\n",
        "\n",
        "For each prompt, record:\n",
        "\n",
        "Prompt text\n",
        "\n",
        "Sampling parameters used\n",
        "\n",
        "Generated output"
      ],
      "metadata": {
        "id": "n7Mfr4ZhcTsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q4 Analysis & Reflection:\n",
        "In your submission, include a short written analysis addressing:\n",
        "\n",
        "A. (6 points) Sampling Behavior\n",
        "(2 points) How does increasing temperature affect coherence?\n",
        "\n",
        "(2 points) How does changing top-k affect repetition or diversity?\n",
        "\n",
        "(2 points) Which settings produced the “best” outputs, and why?\n",
        "\n",
        "B. (10 points) Model Limitations\n",
        "(8 points) Identify at least one failure mode, such as:\n",
        "\n",
        "Repetitive loops\n",
        "\n",
        "Loss of grammatical structure\n",
        "\n",
        "Sudden topic drift\n",
        "\n",
        "Nonsensical phrasing\n",
        "\n",
        "(2 points) Explain why this happens in a small, single-block GPT model.\n",
        "\n",
        "C. (6 points) Architectural Reflection\n",
        "Briefly discuss how model size and training data limitations affect:\n",
        "\n",
        "(2 points) Long-range coherence\n",
        "\n",
        "(2 points) Semantic consistency\n",
        "\n",
        "(2 points) Real-world usability"
      ],
      "metadata": {
        "id": "R9gwahF3cfpp"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_with_miniature_gpt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}