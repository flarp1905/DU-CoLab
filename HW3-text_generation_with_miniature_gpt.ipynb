{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZxD7N0kzqWZ"
      },
      "source": [
        "# Text generation with a miniature GPT\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/29<br>\n",
        "**Last modified:** 2020/05/29<br>\n",
        "**Description:** Implement a miniature version of GPT and train it to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNVBaTr2zqWg"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement an autoregressive language model\n",
        "using a miniature version of the GPT model.\n",
        "The model consists of a single Transformer block with causal masking\n",
        "in its attention layer.\n",
        "We use the text from the IMDB sentiment classification dataset for training\n",
        "and generate new movie reviews for a given prompt.\n",
        "When using this script with your own dataset, make sure it has at least\n",
        "1 million words.\n",
        "\n",
        "This example should be run with `tf-nightly>=2.3.0-dev20200531` or\n",
        "with TensorFlow 2.3 or higher.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n",
        "- [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "- [GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik5oNo61zqWj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4UMCqw9LzqWl"
      },
      "outputs": [],
      "source": [
        "# We set the backend to TensorFlow. The code works with\n",
        "# both `tensorflow` and `torch`. It does not work with JAX\n",
        "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
        "# (used in `causal_attention_mask()`: `tile` in JAX does\n",
        "# not support a dynamic `reps` argument.\n",
        "# You can make the code work in JAX by wrapping the\n",
        "# inside of the `causal_attention_mask` function in\n",
        "# a decorator to prevent jit compilation:\n",
        "# `with jax.ensure_compile_time_eval():`.\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import tensorflow\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNTLYuawzqWo"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zqNah_O9zqWp"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = ops.arange(n_dest)[:, None]\n",
        "    j = ops.arange(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = ops.cast(m, dtype)\n",
        "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = ops.concatenate(\n",
        "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
        "    )\n",
        "    return ops.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6SXuEOuzqWq"
      },
      "source": [
        "## Implement an embedding layer\n",
        "\n",
        "Create two separate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "34q9patnzqWr"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(0, maxlen, 1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knP90MMezqWt"
      },
      "source": [
        "## Implement the miniature GPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q7sRdGSUzqWu"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\",\n",
        "        loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff-sz9GrzqWv"
      },
      "source": [
        "## Prepare the data for word-level language modelling\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4opVmskzqWv",
        "outputId": "f795868b-e04e-4fde-95d3-bb58ab4ac13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  6085k      0  0:00:13  0:00:13 --:--:-- 13.5M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0D8IaHCzqWv",
        "outputId": "171e39bd-6a7c-47c2-a755-e04e8dc92272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "# Create a dataset from text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf_data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf_strings.lower(input_string)\n",
        "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tensorflow.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80pvURAtzqWw"
      },
      "source": [
        "## Implement a Keras callback for generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VZhzryQnzqWw"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x, verbose=0)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHzYzCGkzqWy"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5inuJrzqWz",
        "outputId": "93567b2b-24af-435d-bfe5-a934c41e4a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text:\n",
            "this movie is just about the most of the other movies , and i have seen , this film . the plot is very much of the same , but it is a [UNK] of a lot of the [UNK] and more than a\n",
            "\n",
            "391/391 - 79s - 202ms/step - loss: 5.4469\n",
            "Epoch 2/25\n",
            "generated text:\n",
            "this movie is a very good example of the film , with a lot of great acting in the movie . . the movie [UNK] , [UNK] [UNK] the [UNK] ) , [UNK] ) , [UNK] . it 's a [UNK] ' of [UNK]\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 4.6928\n",
            "Epoch 3/25\n",
            "generated text:\n",
            "this movie is an enjoyable , and is it 's the [UNK] is the greatest of all time in the 80 's , yet another . i have seen many of them since i have seen the [UNK] \" , and i have to\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 4.4498\n",
            "Epoch 4/25\n",
            "generated text:\n",
            "this movie is a waste of celluloid trash , i love [UNK] , [UNK] \" is a movie , and it has no redeeming value . the acting is great , but the direction is a great movie that it 's the best of\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 4.2952\n",
            "Epoch 5/25\n",
            "generated text:\n",
            "this movie is a shame that this movie should have been made in [UNK] , not only in my life i was looking forward to see . it was a lot worse than the other two films . the first time they had a\n",
            "\n",
            "391/391 - 62s - 160ms/step - loss: 4.1765\n",
            "Epoch 6/25\n",
            "generated text:\n",
            "this movie is very interesting . there are a few things that i think that you can see it , and then , the acting is absolutely amazing ! the plot is a simple and the movie has a lot of holes you can\n",
            "\n",
            "391/391 - 64s - 162ms/step - loss: 4.0796\n",
            "Epoch 7/25\n",
            "generated text:\n",
            "this movie is about 2 [UNK] the dolls \" of a [UNK] . the movie starts , with a great [UNK] ) , an interesting premise that has a [UNK] going through his hands . it becomes a bit slow . the acting is\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.9961\n",
            "Epoch 8/25\n",
            "generated text:\n",
            "this movie is a rip off of the [UNK] movie is not bad . but it is one of my favorite movies . i can 't think of it is . [UNK] the plot is a joke , but the movie has everything that\n",
            "\n",
            "391/391 - 62s - 160ms/step - loss: 3.9229\n",
            "Epoch 9/25\n",
            "generated text:\n",
            "this movie is not a good movie for all the romantics around . it is great and the characters were all about , the acting was great , the plot , and some good parts were pretty good and the movie did a good\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.8583\n",
            "Epoch 10/25\n",
            "generated text:\n",
            "this movie is an enjoyable and entertaining , funny movie , i think it 's funny . it 's not funny . the first scene after all , you see [UNK] [UNK] \" , [UNK] [UNK] [UNK] is a bit of [UNK] and you\n",
            "\n",
            "391/391 - 63s - 162ms/step - loss: 3.7999\n",
            "Epoch 11/25\n",
            "generated text:\n",
            "this movie is the only time i am bothering to explain this crap and the other comment i have ever seen it since the first time of all , i can say , this is a movie that is based off on the [UNK]\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.7474\n",
            "Epoch 12/25\n",
            "generated text:\n",
            "this movie is a good movie . the plot is very good , and is very good , and you will love it . you 'll know what you see here is a movie that you are in the middle class system that makes\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.6990\n",
            "Epoch 13/25\n",
            "generated text:\n",
            "this movie is not bad enough to watch it . the characters behave like animals , and characters . i don 't feel that the plot . i 'm a bit disappointed . a whole movie is not just plain good enough to see\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.6560\n",
            "Epoch 14/25\n",
            "generated text:\n",
            "this movie is an insult to any viewer 's intelligence , not even the worst . i have seen some kind of [UNK] , it was actually quite funny but i 'm not sure what the [UNK] of the characters they are so stupid\n",
            "\n",
            "391/391 - 63s - 162ms/step - loss: 3.6160\n",
            "Epoch 15/25\n",
            "generated text:\n",
            "this movie is a very funny movie about three guys , who are [UNK] and the most stupid [UNK] in a movie . if you think you are going to laugh at the end . it is a waste of money and a wanna\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.5783\n",
            "Epoch 16/25\n",
            "generated text:\n",
            "this movie is a great concept . a film with a great concept , good acting , story , and some people who think they had been trying to live a new job . the film is a good one . it has to\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.5438\n",
            "Epoch 17/25\n",
            "generated text:\n",
            "this movie is a complete waste of time . this movie is one of the worst movies i 've seen in my life . it 's like an hour and a half , but i think it 's very funny and the characters are\n",
            "\n",
            "391/391 - 62s - 160ms/step - loss: 3.5117\n",
            "Epoch 18/25\n",
            "generated text:\n",
            "this movie is a very good movie . it 's a real feel . the characters are very interesting . i would recommend this movie for anyone who likes it . i have never seen it twice . it is a great [UNK] ,\n",
            "\n",
            "391/391 - 64s - 163ms/step - loss: 3.4820\n",
            "Epoch 19/25\n",
            "generated text:\n",
            "this movie is a total waste of time . i was actually surprised by the actors , but i can 't understand why the plot was so predictable i was very disappointed . the story is about the acting of the plot . the\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.4540\n",
            "Epoch 20/25\n",
            "generated text:\n",
            "this movie is absolutely horrible and the acting is absolutely atrocious in the movie . the plot is just so poor it is just plain boring and poorly written . it 's just a waste of time ! it is very funny , but\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.4274\n",
            "Epoch 21/25\n",
            "generated text:\n",
            "this movie is one of my favourite horror movies , and even though the plot is so bad they have done it ! the movie starts with a [UNK] and the [UNK] 's [UNK] of the worst . the characters are just plain and\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.4029\n",
            "Epoch 22/25\n",
            "generated text:\n",
            "this movie is a great example of how it affects a film and has very end to be able to endure the entire story line , a bunch of people who decide to make it out , a movie , and they don 't\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.3799\n",
            "Epoch 23/25\n",
            "generated text:\n",
            "this movie is really a low standard movie , that 's the book is much better than this movie . it is a mess of a film and is very poorly written and poorly directed . it 's also a very good thing .\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.3586\n",
            "Epoch 24/25\n",
            "generated text:\n",
            "this movie is a classic . if you are not familiar with the story , the characters and some very good acting . there is very little plot , you don 't get a good part and you 'll love it . if you\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.3379\n",
            "Epoch 25/25\n",
            "generated text:\n",
            "this movie is a very entertaining rip -off of underworld that i don 't like a movie . it 's a great deal about the movie , that is not so funny . it is a good movie ! it 's all that is\n",
            "\n",
            "391/391 - 63s - 162ms/step - loss: 3.3188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cdc2b1d78c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = create_model()\n",
        "\n",
        "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc9OMx0pzqW3"
      },
      "source": [
        "## Relevant Chapters from Deep Learning with Python\n",
        "- [Chapter 15: Language models and the Transformer](https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer)\n",
        "- [Chapter 16: Text generation](https://deeplearningwithpython.io/chapters/chapter16_text-generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q1 Option A — Function-Based Text Generation:\n",
        "Implement a reusable function (e.g., generate_text(...)) that:\n",
        "* Accepts a text prompt\n",
        "* Generates a configurable number of new tokens\n",
        "* Uses probabilistic sampling (not argmax)\n",
        "* Returns generated text as a string\n",
        "\n",
        "You should be able to call it like:\n",
        "* generate_text(model, \"this movie is\", num_tokens=60, temperature=0.8, top_k=20)"
      ],
      "metadata": {
        "id": "P7R7A01e-J9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating text on demand\n",
        "def generate_text(model, prompt, max_tokens=60, temperature=0.8, top_k=20):\n",
        "  \"\"\"\n",
        "  Option A — Function-Based Text Generation (Minimum Requirement)\n",
        "  Implement a reusable function (e.g., generate_text(...)) that:\n",
        "  * Accepts a text prompt\n",
        "  * Generates a configurable number of new tokens\n",
        "  * Uses probabilistic sampling (not argmax)\n",
        "  * Returns generated text as a string\n",
        "  \"\"\"\n",
        "  #starts the tokenization process (prompt into consumable bits for model)\n",
        "  start_tokens = [word_to_index.get(word, 1) for word in prompt.split()]\n",
        "  num_tokens_generated = 0 #iniliazes tokens\n",
        "  tokens_generated = [] #stores the tokens\n",
        "\n",
        "  while num_tokens_generated <= max_tokens:\n",
        "    pad_len = maxlen - len(start_tokens)\n",
        "    sample_index = len(start_tokens) - 1\n",
        "    if pad_len < 0:\n",
        "      x = start_tokens[:maxlen]\n",
        "      sample_index = maxlen - 1\n",
        "    elif pad_len > 0:\n",
        "      x = start_tokens + [0] * pad_len\n",
        "    else:\n",
        "      x = start_tokens\n",
        "    x = np.array([x])\n",
        "    y, _ = model.predict(x, verbose=0)\n",
        "\n",
        "    #add in temperature - how consistent this will be (lower=more; high=less)\n",
        "    logits = y[0][sample_index] / temperature\n",
        "    #add in top-k part of predicitive maths the \"most\" likely next token k out of the pile\n",
        "    logits, indices = ops.top_k(logits, k=top_k, sorted=True)\n",
        "    indices = np.asarray(indices).astype(\"int32\")\n",
        "    preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype(\"float32\")\n",
        "\n",
        "    sample_token = np.random.choice(indices, p=preds)\n",
        "    tokens_generated.append(sample_token)\n",
        "    start_tokens.append(sample_token)\n",
        "    num_tokens_generated += 1\n",
        "\n",
        "  generated_text = [vocab[token] for token in tokens_generated]\n",
        "  return prompt + \" \" + \" \".join(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "5oA-YzUB-H3A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q2 Sampling Controls:\n",
        "Your interaction must expose at least two of the following parameters:\n",
        "\n",
        "Parameter\tDescription\n",
        "* Temperature\tControls randomness. Lower = safer, higher = more creative\n",
        "* Top-k\tRestricts sampling to the top-k most likely tokens\n",
        "* Max tokens\tNumber of tokens generated beyond the prompt\n",
        "* You must demonstrate that changing these parameters affects output behavior."
      ],
      "metadata": {
        "id": "DftcB23Qb9oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate different answers with different parameters\n",
        "prompt = \"I like this movie because\"\n",
        "\n",
        "print(\"\\nChange in Temperature changes how consistent responses will be. Default was set to 80%.\")\n",
        "print(\"\")\n",
        "for i in range(3):\n",
        "  print(f\"Temperature at .8 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=20)}\")\n",
        "for i in range(3):\n",
        "  print(f\"Temperature at .2 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.2, top_k=20)}\")\n",
        "\n",
        "print(\"\\nChange in Top-K changes how much variety in word choice there is. Default was set to 20.\")\n",
        "print(\"\")\n",
        "for i in range(3):\n",
        "  print(f\"Top-k at 20 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=20)}\")\n",
        "for i in range(3):\n",
        "  print(f\"Top-k at 2 {i + 1}: {generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=2)}\")\n",
        "\n",
        "print(\"\\nThis section has four results of mixing the variable pool from above.\")\n",
        "print(\"Testing low temp & low top-k\")\n",
        "print(f\"{generate_text(model, prompt, max_tokens=60, temperature=.2, top_k=2)}\")\n",
        "print(\"\\nTesting high temp & high top-k\")\n",
        "print(f\"{generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=20)}\")\n",
        "print(\"\\nTesting low temp & high top-k\")\n",
        "print(f\"{generate_text(model, prompt, max_tokens=60, temperature=.2, top_k=20)}\")\n",
        "print(\"\\nTesting high temp & low top-k\")\n",
        "print(f\"{generate_text(model, prompt, max_tokens=60, temperature=.8, top_k=2)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzgZHxHEs8YO",
        "outputId": "eec36824-d95f-4f52-e389-4dc936ce85cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Change in Temperature changes how consistent responses will be. Default was set to 80%.\n",
            "\n",
            "Temperature at .8 1: I like this movie because its a [UNK] . the acting is a good plot , the characters are very well done , and the characters are likable . they all they have done their life on their own . they are just very annoying . i guess the fact that they are really bad guys . it 's not very boring . all they are\n",
            "Temperature at .8 2: I like this movie because it is a true story of the fictional characters of the fictional . they are very very good , and the characters are not shallow . the movie had a nice girl in the story . the acting is weak and the dialog are weak . this was the story line the dialogue was weak . it was very slow and\n",
            "Temperature at .8 3: I like this movie because the script is so monotone . it is indistinguishable from [UNK] - [UNK] [UNK] . however , after watching this movie , most of the music score is [UNK] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Temperature at .2 1: I like this movie because it is a good movie . it is a good movie . it is a good movie about a young girl who is raped and she 's a guy who likes to kill her . she is a guy , and is a guy . i love the girl who is a guy , and is a guy , and is\n",
            "Temperature at .2 2: I like this movie because it is a very good movie . it is a good movie . it is a very good movie . it is a good movie , but it is a good movie . it is a good movie . it is a good movie , and it is a good movie . it is a good movie , but it 's\n",
            "Temperature at .2 3: I like this movie because it is a great movie . it is a very good movie , and it is a very good movie . it is very good and i have to say that it is very good . it is very good , very good . it is very good , very good and i think it is very good .  \n",
            "\n",
            "Change in Top-K changes how much variety in word choice there is. Default was set to 20.\n",
            "\n",
            "Top-k at 20 1: I like this movie because it takes us a few years back in 2000 . this movie has no [UNK] whatsoever , a plot . i have no idea what it was about . the plot is a joke in some of the most dangerous animal movies i have seen . the actors are ok , a good movie . [UNK] in fact , no matter\n",
            "Top-k at 20 2: I like this movie because it is a bad movie that you would be like to be fooled , this movie , it is not bad , it is a movie that it is actually interesting . the plot is very similar . it doesn 't happen in a lot of other movies like this one . the only thing that i don 't like it\n",
            "Top-k at 20 3: I like this movie because it 's based on a true real life story . it has a few flaws that is still a bit good about it . but it is just too bad and it is a story that will be too preachy and hackneyed . the acting is a bit irritating . but still i would recommend that children just to watch this\n",
            "Top-k at 2 1: I like this movie because it is a good movie , it is not a good one . it is a good movie . it is not a good movie . it is a good movie , but it is not funny . it is a good movie . the acting is great , the plot is great , the acting is great , the plot\n",
            "Top-k at 2 2: I like this movie because it is not a good movie . it is a very good movie . it is very good and i don 't know what it is . it is a very good movie , and i think it 's a very good movie , but i can say is that it is very good , but i think it 's a\n",
            "Top-k at 2 3: I like this movie because i am not a fan of the first time i was so disappointed that i was really disappointed in this movie . i think the story is a very good movie . i think it was very good , and i think it was a good thing . i was wrong . i was very disappointed in this movie , but\n",
            "\n",
            "This section has four results of mixing the variable pool from above.\n",
            "Testing low temp & low top-k\n",
            "I like this movie because it is a good movie . it 's a good movie . it 's a good movie . it 's not a good movie , but it 's a good movie . it 's not good to say , it 's a good movie , but it 's not a good movie . it 's not a good movie , but\n",
            "\n",
            "Testing high temp & high top-k\n",
            "I like this movie because it 's a bad movie for it . it 's so bad you can not understand what it got to go . if you 're a fan of michael dudikoff , lisa howard . i love him , this is a respectable movie . while the writers were in [UNK] ' - -they 're all wrong with good actors spouting this\n",
            "\n",
            "Testing low temp & high top-k\n",
            "I like this movie because it is a very good movie . it is a very good movie . it is very good . i think it is a good movie . it is a very good movie . it is a very good movie . the acting is very good , the acting is very good , and the plot is very good . it\n",
            "\n",
            "Testing high temp & low top-k\n",
            "I like this movie because i am a huge fan of [UNK] [UNK] and i love it . i love it . it is a great movie , and i have to say i love the first time and i love the movie . i love the first half of the movie , but it was very funny . i love it . i love the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q3 Prompt Exploration:\n",
        "You must test your model with at least 10 distinct prompts, including:\n",
        "* Short prompts (2–4 words)\n",
        "* Medium prompts (5–8 words)\n",
        "* At least one ambiguous or incomplete prompt\n",
        "\n",
        "For each prompt, record:\n",
        "* Prompt text\n",
        "* Sampling parameters used\n",
        "* Generated output"
      ],
      "metadata": {
        "id": "n7Mfr4ZhcTsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate different answers with ten different prompts\n",
        "# short (2-4 word) prompts x4\n",
        "prompt_short = [\n",
        "    \"This movie rocked\",\n",
        "    \"This movie sucked\",\n",
        "    \"I wasn't impressed by\",\n",
        "    \"The acting was\"\n",
        "]\n",
        "# medium (5-8 word) prompts x4\n",
        "prompt_medium = [\n",
        "    \"In act two when they did\",\n",
        "    \"The character growth in the partner was\",\n",
        "    \"The movie is fun to look at\", #from my six year old\n",
        "    \"The horses seemed too fast to me\" #from my nine year old\n",
        "]\n",
        "# ambiguous prompts x2\n",
        "prompt_ambiguous = [\n",
        "    \"When the dragon showed up I was\",\n",
        "    \"but the butler butter ba\",\n",
        "]\n",
        "\n",
        "print(\"\\nChanges to the prompts will cause changes to the outputs.\")\n",
        "print(\"In this section there are three prompt types four short (2-4 word), four medium (5-8 word), and two ambiguous.\")\n",
        "print(\"All other variables are the same default values allowing full focus to be placed on the prompt effect.\")\n",
        "print(\"Default values are: generate_text(model, '<our prompts>', num_tokens=60, temperature=0.8, top_k=20) \")\n",
        "print(\"\")\n",
        "\n",
        "print(\"\\nShort Prompts (2-4 words).\")\n",
        "for i in prompt_short:\n",
        "  print(f\"Prompting with '{i}': {generate_text(model, i, max_tokens=60, temperature=0.8, top_k=20)}\")\n",
        "\n",
        "print(\"\\nMedium Prompts (5-8 words).\")\n",
        "for i in prompt_medium:\n",
        "  print(f\"Prompting with '{i}': {generate_text(model, i, max_tokens=60, temperature=0.8, top_k=20)}\")\n",
        "\n",
        "print(\"\\nAmbiguous Prompts.\")\n",
        "for i in prompt_ambiguous:\n",
        "  print(f\"Prompting with '{i}': {generate_text(model, i, max_tokens=60, temperature=0.8, top_k=20)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6g4hU4EWgY8",
        "outputId": "25302085-4b72-463a-a71a-616169b54905"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Changes to the prompts will cause changes to the outputs.\n",
            "In this section there are three prompt types four short (2-4 word), four medium (5-8 word), and two ambiguous.\n",
            "All other variables are the same default values allowing full focus to be placed on the prompt effect.\n",
            "Default values are: generate_text(model, '<our prompts>', num_tokens=60, temperature=0.8, top_k=20) \n",
            "\n",
            "\n",
            "Short Promts (2-4 words).\n",
            "Prompting with 'This movie rocked': This movie rocked the dark night of [UNK] was the lights came to the screen , but this one was not so [UNK] that we 've seen . . . . .all in a good way , i 'm a little nervous about it . there 's really no reason to watch it . . . . . . . . this is the\n",
            "Prompting with 'This movie sucked': This movie sucked ! ! ! ! ! i love the acting was very poor and the plot is so weak and the plot [UNK] ! ! i mean , who has the same plot , how i actually gave this movie a better script . the plot is simple , but it is a very nice , and a very nice little too\n",
            "Prompting with 'I wasn't impressed by': I wasn't impressed by this film is directed by michael jackson . a haunting quincy jones score , but this is a tremendous musical & [UNK] musical score . i was very disappointed by score that is a musical score from blake edwards ' classic song with a score , prince songs , levant , foch , and caron who is often found out of\n",
            "Prompting with 'The acting was': The acting was good , and a good thing . all you can say . it 's not good , but i feel this movie doesn 't need to be any of the good , but it is a bit cheesy , and it doesn 't deliver smoothly into the story . the story is solid and it is [UNK] in fact that ,\n",
            "\n",
            "Medium Promts (5-8 words).\n",
            "Prompting with 'In act two when they did': In act two when they did [UNK] the movie makers of this movie - they were not just the fact that it 's supposed to be funny . they were all the way to make it . it is , and it was an insult to my intelligence . the characters were funny and the movie just plain and simple . it is a good movie .\n",
            "Prompting with 'The character growth in the partner was': The character growth in the partner was [UNK] [UNK] and [UNK] . portia de [UNK] , as the queen of [UNK] . [UNK] is [UNK] 's [UNK] . he 's [UNK] 's [UNK] [UNK] to [UNK] in the [UNK] of the ring . he [UNK] [UNK] [UNK] with the [UNK] , he intends to focus on the unfolding of his narrative , the narrative is facing an outsider\n",
            "Prompting with 'The movie is fun to look at': The movie is fun to look at a high school student who 's [UNK] school , [UNK] . \" it is a geek by the sorority girls who , decide to start a fight with an old girlfriend who slums in rio , during a party , he 's a construction resort a new york couple of the local boys club , his girlfriend beth stabbed with the\n",
            "Prompting with 'The horses seemed too fast to me': The horses seemed too fast to me , but i got the first two parts of this movie were just so frustrating because it is a combination of two hours tracking shots , etc . but it 's not a good story . there 's some interesting scenes involving bestiality and a taboo subject . a lot of their various times , but it takes a bit of\n",
            "\n",
            "Ambiguous Promts.\n",
            "Prompting with 'When the dragon showed up I was': When the dragon showed up I was really the movie that i saw in germany and didn 't even get it . it was funny , a shame that it was a shame to make no longer and the best friend of mine . it was not funny . this is very funny and the special effects , and the acting were very good .   \n",
            "Prompting with 'but the butler butter ba': but the butler butter ba milk . it makes the funny line and a [UNK] humour that is undeniably funny . it 's humour that [UNK] humour is more than a joke that 's a funny giggle inducing . the entire characters in the entire show is about two characters , and the adventures of the show have been funnier films . it has been funnier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Q4 Analysis & Reflection:\n",
        "In your submission, include a short-written analysis addressing:\n",
        "\n",
        "A. (6 points) Sampling Behavior\n",
        "* (2 points) How does increasing temperature affect coherence?\n",
        "\tTemperature affects the randomness of the output. Lower temperatures reduce the flexibility of the model leading to more predictable results. Higher temperatures are more flexible and creative. Thus this can be used between blocks to help generate both creative AND coherent responses. Too low of a temperature leads to significant looping effects while too high might produce nonsense.\n",
        "\n",
        "* (2 points) How does changing top-k affect repetition or diversity?\n",
        "\tTop-K impacts the pool of available tokens (words in this case) that can be selected from for the predicative model. A smaller pool gives less ability for the model to be creative or be successful in avoiding the failure modes.\n",
        "\n",
        "* (2 points) Which settings produced the “best” outputs, and why?\n",
        "\tTo better answer this question, I went back to Question 2 to directly compare the two variables. Both being set to high created the “best” answers – however, without a larger data set OR more layers it still created some…interestingly odd results.\n",
        "\n",
        "B. (10 points) Model Limitations\n",
        "* (8 points) Identify at least one failure mode, such as:\n",
        "* Repetitive loops – there were multiple failure loops with strings of ‘!’ and ‘.’ as well as word choices.\n",
        "* Loss of grammatical structure – I think this was shown when the model outputted [UNK] tokens.\n",
        "* Sudden topic drift – There were a few of these, but the most obvious was the instance off of “the horses seemed too fast to me” prompt suddenly shifting to talk about bestiality.\n",
        "* Nonsensical phrasing – “In act two when they did” led to a string of random seeming sentences.\n",
        "\n",
        "* (2 points) Explain why this happens in a small, single-block GPT model.\n",
        "\tThe reasons single-block isn’t used is demonstrated succinctly in this project. Without feeding it’s outputs into the next block(s) to gain refinement for the eventual output, users are left with…this rubbish. Multiple blocks would catch all four of the failure modes listed above.\n",
        "\n",
        "C. (6 points) Architectural Reflection\n",
        "Briefly discuss how model size and training data limitations affect:\n",
        "\t(all three share very similar answers based on this being a model for educational purpose…)\n",
        "* (2 points) Long-range coherence\n",
        "\tLong-range coherence requires a significant amount of data to train on, as well as multiple blocks to enable refinement of responses; neither of which were attempted in this homework. Without multiple linked blocks the model cannot catch its own significant errors caused by variables in the generator. While the ingested data set was far too small to provide sufficient training.\n",
        "\n",
        "* (2 points) Semantic consistency\n",
        "\tRequires more training data and the multiple blocks for refinement.\n",
        "\n",
        "* (2 points) Real-world usability\n",
        "\tThis model, as is, is not usable in the world outside of rapidly assisting students in understanding what the parameters and prompts do within an LLM. If this was to be used in a real-world scenario otherwise, it would cause confusion, anger, and potential lawsuits.\n"
      ],
      "metadata": {
        "id": "IIZGxHZTWg9R"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_with_miniature_gpt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}